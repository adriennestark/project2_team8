{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Stock Predictor Using Closing Prices\n",
    "In this notebook, you will build and train a custom LSTM RNN that uses a 10 day window of Carnival Stock prices closing prices to predict the 11th day closing price.\n",
    "\n",
    "We are:\n",
    "\n",
    "Preparing the data for training and testing Building and train a custom LSTM RNN Evaluating the performance of the model\n",
    "\n",
    "# Data Preparation\n",
    "In this section, you will need to prepare the training and testing data for the model. The model will use a rolling 10 day window to predict the 11th day closing price.\n",
    "\n",
    "You will need to:\n",
    "\n",
    "Use the window_data function to generate the X and y values for the model. Split the data into 70% training and 30% testing Apply the MinMaxScaler to the X and y values Reshape the X_train and X_test data for the model. Note: The required input format for the LSTM is: reshape((X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random seed for reproducibility\n",
    "# Note: It is good practice to comment this out and run multiple experiments to evaluate your model\n",
    "# ask Jeff random seed 1 vs set seed 2\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import random\n",
    "random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Volatility index data\n",
    "df = pd.read_csv('stock_data/vix.csv', index_col=\"Date\", infer_datetime_format=True, parse_dates=True)\n",
    "df = df.drop(columns= \"Open\")\n",
    "df = df.rename(columns={'Close': 'vix_value'})\n",
    "df['vix_value']=df['vix_value'].astype(int) \n",
    "df.index = df.index.normalize()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the historical Volatility index data to Covid dates\n",
    "df_sliced = df.loc['2020-03-02':'2021-12-30']\n",
    "df_sliced.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the historical closing prices for Carnival\n",
    "df2 = pd.read_csv('stock_data/ccl.csv', index_col=\"Date\", infer_datetime_format=True, parse_dates=True)['Close']\n",
    "df2 = df2.sort_index()\n",
    "df2.index = df2.index.normalize()\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the historical closing prices for Carnival Cruises to Covid dates\n",
    "df2_sliced = df2['2020-03-02':'2020-12-30']\n",
    "df2_sliced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the data into a single DataFrame\n",
    "df_sliced = df_sliced.join(df2_sliced, how=\"inner\")\n",
    "df_sliced.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sliced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accepts the column number for the features (X) and the target (y)\n",
    "# It chunks the data up with a rolling window of Xt-n to predict Xt\n",
    "# It returns a numpy array of X any y\n",
    "def window_data(df_sliced, window, feature_col_number, target_col_number):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(df_sliced) - window - 1):\n",
    "        features = df_sliced.iloc[i:(i + window), feature_col_number]\n",
    "        target = df_sliced.iloc[(i + window), target_col_number]\n",
    "        X.append(features)\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Closing Prices using a 200 day window of previous closing prices\n",
    "# Then, experimenting with window sizes anywhere from 1 to 10 and see how the model performance changes\n",
    "window_size = 5\n",
    "\n",
    "# Column index 0 is the 'vix_value' column\n",
    "# Column index 1 is the `Close` column\n",
    "feature_column = 1\n",
    "target_column = 1\n",
    "X, y = window_data(df_sliced, window_size, feature_column, target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 70% of the data for training and the remaineder for testing\n",
    "split = int(0.7 * len(X))\n",
    "\n",
    "X_train = X[: split]\n",
    "X_test = X[split:]\n",
    "\n",
    "y_train = y[: split]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Using the MinMaxScaler to scale data between 0 and 1.\n",
    "# Creating a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "# Fitting the MinMaxScaler object with the features data X\n",
    "scaler.fit(X)\n",
    "\n",
    "# Scaling the features training and testing sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fitting the MinMaxScaler object with the target data Y\n",
    "scaler.fit(y)\n",
    "\n",
    "# Scaling the target training and testing sets\n",
    "y_train_scaled = scaler.transform(y_train)\n",
    "y_test_scaled = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the features for the model\n",
    "X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "# Printing some sample data after reshaping the datasets\n",
    "print (f\"X_train sample values:\\n{X_train_scaled[:3]} \\n\")\n",
    "print (f\"X_test sample values:\\n{X_test_scaled[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train the LSTM RNN\n",
    "In this section, you will design a custom LSTM RNN and fit (train) it using the training data.\n",
    "\n",
    "You will need to:\n",
    "\n",
    "Define the model architecture Compile the model Fit the model to the training data\n",
    "\n",
    "# Hints:\n",
    "\n",
    "You will want to use the same model architecture and random seed for both notebooks. This is necessary to accurately compare the performance of the Vix model vs the closing price model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the LSTM model. \n",
    "# The return sequences need to be set to True if you are adding additional LSTM layers, but \n",
    "# No need to do this for the final layer. \n",
    "# Note: The dropouts help prevent overfitting\n",
    "# Note: The input shape is the number of time steps and the number of indicators\n",
    "# Note: Batching inputs has a different input shape of Samples/TimeSteps/Features\n",
    "\n",
    "# Defining the LSTM RNN model.\n",
    "model = Sequential()\n",
    "\n",
    "# Initial model setup\n",
    "number_units = 30\n",
    "dropout_fraction = 0.2\n",
    "\n",
    "# Layer 1\n",
    "model.add(LSTM(\n",
    "    units=number_units,\n",
    "    return_sequences=True,\n",
    "    input_shape=(X_train_scaled.shape[1], 1)))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Layer 2\n",
    "model.add(LSTM(units=number_units, return_sequences=True))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Layer 3\n",
    "model.add(LSTM(units=number_units))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "# Using at least 10 epochs\n",
    "# Do not shuffle the data\n",
    "# Experiementing with the batch size, but a smaller batch size is recommended\n",
    "model.fit(X_train_scaled, y_train_scaled, epochs=10, shuffle=False, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance\n",
    "In this section, you will evaluate the model using the test data.\n",
    "\n",
    "You will need to:\n",
    "\n",
    "1. Evaluate the model using the X_test and y_test data.\n",
    "2. Use the X_test data to make predictions\n",
    "3. Create a DataFrame of Real (y_test) vs predicted values.\n",
    "4. Plot the Real vs predicted values as a line chart\n",
    "\n",
    "### Hints\n",
    "Remember to apply the inverse_transform function to the predicted and y_test values to recover the actual closing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "model.evaluate(X_test_scaled, y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making some predictions\n",
    "predicted = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovering the original prices instead of the scaled version\n",
    "predicted_prices = scaler.inverse_transform(predicted)\n",
    "real_prices = scaler.inverse_transform(y_test_scaled.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame of Real and Predicted values\n",
    "stocks = pd.DataFrame({\n",
    "    \"Real\": real_prices.ravel(),\n",
    "    \"Predicted\": predicted_prices.ravel()\n",
    "}, index = df_sliced.index[-len(real_prices): ]) \n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the real vs predicted values as a line chart\n",
    "stocks.plot(title=\"Real Vs. Predicted Prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
